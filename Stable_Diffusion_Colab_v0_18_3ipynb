{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/loopyd/NOP-SD/blob/main/Stable_Diffusion_Colab_v0_18_3ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# NOP-SD Stable Diffusion Colab v0.18"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Welcome to my stabe-diffusion lab notebook!\n",
        "\n",
        "This Google Collab is a:\n",
        "\n",
        "* One-stop notebook for various stable-diffusion features you can test-drive.\n",
        "* A Goto guide that can help you understand additional stable-diffusion concepts and how to apply them yourself.\n",
        "* A sample of some of our community contributions that have made this Open Source project what it is today!\n",
        "-------------\n",
        "## What is Stable Diffusion?\n",
        "\n",
        "To learn more about stable diffusion, you can read [this article](https://huggingface.co/blog/stable_diffusion) on huggingface.\n",
        "\n",
        "-------\n",
        "## Changelog\n",
        "\n",
        "| Version | Changes         | \n",
        "| ------- | --------------- |\n",
        "| **v0.10** | Colab creation |\n",
        "| **v0.11** | Google Drive option for TXT2IMG and some error corrections |\n",
        "| **v0.12** | Added more options to TXT2IMG |\n",
        "| **v0.13** | Diffusers added a feature which broke the pipeline with the current implementation, reverted back to an older version |\n",
        "| **v0.14** | Added in full precision in the diffuser method |\n",
        "| **v0.15** | Added in file saving in drive for diffusers |\n",
        "| **v0.16** | Added in prompt saving |\n",
        "| **v0.17** | Added in the new weights and disabled the NSFW check |\n",
        "| **v0.18** | Added in NSFW notices and NSFW toggle switch, improved docs, refactored some cells, adjusted ranges for UI sliders to within effective constraints. |\n",
        "\n",
        "-------\n",
        "## TODO\n",
        "- Saving weights in Google Drive\n",
        "- Add in the optimized Stable Diffusion fork ( https://github.com/basujindal/stable-diffusion )\n",
        "- Implement https://github.com/DamascusGit/stable-diffusion/blob/main/scripts/txt2img_k.py\n",
        "- Add offline install instructions (50% done)\n",
        "- Add in k-diffusion\n",
        "- Add in inpainting\n",
        "- Add init functionality\n",
        "- Several prompts in one run\n",
        "- Add in an upscaler\n",
        "- Add in a plms, ddim, etc choice (50% done)\n",
        "- Option to load a config file to load in preset settings\n",
        "- More functionality\n",
        "\n",
        "-----------------\n",
        "## Credits\n",
        "Project Maintainer:  NOP#1337\n",
        "\n",
        "Contributors:\n",
        "\n",
        "- Pierre#2575 and Ragnall#1177 for the default prompt\n",
        "- Original TXT2IMG Notebook: Lucas Ferreira da Silva, Madams, Greg Turk\n",
        "- Saber7ooth#7527 for debugging, testing, code contributions.\n",
        "\n",
        "-------------\n",
        "## Feedback\n",
        "\n",
        "If you have suggestions, bug reports, or implementations, feel free to contact me on Discord:  NOP#1337\n",
        "\n",
        "For the sake of my sanity and my time, I will not be giving support on offline installations or non-related troubleshooting.\n",
        "\n",
        "Stable Diffusion github introduced watermarks and an NSFW filter in the public release.  This tool does allow you to disable the NSFW filter by stubbing out the contextual difference vectorization built into the safety_checker module that applies both.  **However**:  underneath the Open Source Creative Commons MIL, I am obligated to direct you to the relevant section of this notebook's documentation.\n",
        "\n",
        "-----------------\n",
        "## Contributions\n",
        "\n",
        "If you'd like to contribute to this project, you can save a working copy of any edits via the File -> **Save a Copy in Drive...** option.  Saving over this notebook, and revision history are both disabled so that I can independently manage change submissions.\n",
        "\n",
        "Send your completed notebook to me, by using the File -> Download -> Download .ipynb option, and then attaching it in a DM to me on Discord, along with an expalantion about what you've done.\n",
        "\n",
        "I will be happy to review contributions when I am able, as long as they fall within the project scope.  If they are appropriate to this project's target, I will merge them with this notebook everyone's currently got access to.\n",
        "\n",
        "------------------------\n",
        "## Setup\n",
        "\n",
        "### 1. Create A Huggingface Account\n",
        "\n",
        "You will need a huggingface account, username, and token to work with this notebook that is authorized ot access the [model weights](https://huggingface.co/CompVis/stable-diffusion) for Standard Diffusion.\n",
        "\n",
        "Please go to:  https://huggingface.co/ and click \"Sign Up\" on the top-right.  You will need to enter your details, verify your e-mail address, and set up your user profile.\n",
        "\n",
        "### 2. Gain access to the Repository.\n",
        "\n",
        "If you haven't already, review the [model card](https://huggingface.co/CompVis/stable-diffusion-v1-4).  In order to gain access to the stable diffusion source code which includes the **model weights**, you will have to accept the [CompVis RAIL License Agreement](https://huggingface.co/spaces/CompVis/stable-diffusion-license), by:\n",
        "\n",
        "1.  Navigating to the (CompVis Organization Page)(https://huggingface.co/CompVis]\n",
        "2.  Clicking on both the `stable-diffusion-v-1-4-original` and `stable-diffusion-v-1-4` pages.\n",
        "3.  Accepting the license agreement by checking the box that indicates that you have done os, and clicking \"Accept Agreement\".\n",
        "4.  You will be granted immediate access to the repository, and can now use it for my notebook.\n",
        "\n",
        "> **NOTICE**:  We notice that a lot of people unfamiliar with huggingface's rather convoluted UI at times get confused here, and don't know how to gain access.  You have to physically visit the Organization Page and click the links to get the accept page to appear.  This is by design, to allow you to explore the project before you make a decision.  Hotlinking doesn't work for good reason.\n",
        "\n",
        "### 3. Create an Access token.\n",
        "\n",
        "Visit:  https://huggingface.co/settings/tokens\n",
        "\n",
        "You will require the User Access Token so that we can authenticate you as somebody who has agreed to the standard-diffusion MIL license agreement and gained access to the model card.  You will need to create one with \"Read\" access.  Click \"New Token\", and a popup will appear.  Give the token any name you want (i. e.: \"Google Collab Token\") and use the drop down to select \"Read\" access, then click \"Generate a Token\".\n",
        "\n",
        "You can now copy the token by putting your mouse over the field on the right, over the \"Copy\" icon (commonly featured in most text editors) and it will be copied to your clipboard.  You can also click the \"eye\" icon to see the token.\n",
        "\n",
        "> **WARNING**:  The token, as it says, provides read access to act as your account.  Please __do not share__ your token with anyone else.  Treat it the same way as your remote login password for huggingface.  Do not show it on your screen when someone else is watching, or you are streaming, and do not share it with others on Discord!\n",
        "\n",
        "### 4.  Want a local copy?\n",
        "\n",
        "If you wish to register completely to download local `.chkpt` files independent of huggingface, you'll need to specify your **GitHub Username** in your Huggingface profile.  Many people are confused by this, because they recieve a `403 Forbidden` error when attempting to download the weights.  After creating your first token and adding your GitHub details to your profile, the change usually takes about 5 minutes to propogate.\n",
        "\n",
        "If you're still getting a ``403 Forbidden`` error, after following all of the steps provided, please let us know on the Discord, and we can help you.\n",
        "\n",
        "### 5.  Have Fun!\n",
        "\n",
        "Once you have a token, you've got your answers for your huggingface username and token when asked for it in the notebook, you can click on the field where it is contained to copy it to your clipboard, and then paste it when asked for it into this notebook.  I recommend keeping the page open in another tab for easy re-runs.\n",
        "\n",
        "For security purposes, you must enter your username and your token each time you run the notebook.\n",
        "\n",
        "https://huggingface.co/docs/hub/security-tokens <-- more info here\n"
      ],
      "metadata": {
        "id": "_Y6RXjS1tTji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pythonic Dependency Injection\n",
        "\n",
        "This cell contains all of the notebook dependencies in a calling function that runs per-cell.  It won't produce any errors if something fails to import.  It just prevents a tone of code spaghetti down the way, and makes it much easier for people learning pytorch to read code that's not been peppered with import definitions.\n",
        "\n",
        "As this is a utility cell, please do not modify its contents unless you know what youa re doing."
      ],
      "metadata": {
        "id": "7XtFSRl1FA18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_imports():\n",
        "  dep_inject = [\n",
        "                    'import torch',\n",
        "                    'import random',\n",
        "                    'import time',\n",
        "                    'import os',\n",
        "                    'import cudacolab',\n",
        "                    'import PIL',\n",
        "                    'import numpy',\n",
        "                    'from PIL import Image',\n",
        "                    'from getpass import getpass',\n",
        "                    'from google.colab import output',\n",
        "                    'from torch import autocast',\n",
        "                    'from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler',\n",
        "                    'from contextlib import contextmanager, nullcontext',\n",
        "                    'from getpass import getpass',\n",
        "                    'from google.colab import drive',\n",
        "                    'from traitlets.traitlets import Long',\n",
        "                    'from sh import mount'\n",
        "                  ]\n",
        "  for inp in dep_inject:\n",
        "    try:\n",
        "      exec(inp)\n",
        "    except ModuleNotFoundError:\n",
        "      print(f\"  The {inp} module failed to import.\")\n",
        "      continue\n",
        "\n",
        "# output.enable_custom_widget_manager()\n"
      ],
      "metadata": {
        "id": "rnSHtvyvRB06"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU Info\n",
        "\n",
        "You can run this cell just to display the GPU information for testing purposes."
      ],
      "metadata": {
        "id": "o5V9MWbFtpNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "_ekR-LW6trWG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76703b0f-dd07-49a1-9ada-2789f94e5407"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Aug 23 02:54:06 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HuggingFace login\n",
        "\n",
        "Here you will be prompted for your Username and your Access Token.  If you do not know how to obtain these, please review my notebook documentation."
      ],
      "metadata": {
        "id": "ZSbsSTkdSQIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_imports\n",
        "\n",
        "!pip install -q condacolab\n",
        "huggingface_username = getpass('Enter your HuggingFace Username: ')\n",
        "huggingface_token = getpass('Enter your HuggingFace Token: ')\n",
        "!echo $huggingface_token | huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "TxpcbOcp_u3V",
        "outputId": "df5a43f9-47dc-4c74-f37e-6331cd79dd33"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-6f99047350ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrun_imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhuggingface_username\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Enter your HuggingFace Username: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mhuggingface_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Enter your HuggingFace Token: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'echo $huggingface_token | huggingface-cli login'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m    844\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m         )\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependencies\n",
        "\n",
        "This cell installs dependencies needed to run stable diffusion"
      ],
      "metadata": {
        "id": "Prg0QcDJID6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_imports\n",
        "\n",
        "#### Install base dependencies here\n",
        "!git lfs install\n",
        "!GIT_LFS_SKIP_SMUDGE=0\n",
        "!pip install transformers pytorch-pretrained-bert spacy ftfy scipy torchmetrics==0.6.0 kornia==0.6\n",
        "!python -m spacy download en\n",
        "!git lfs clone https://$huggingface_username:$huggingface_token@huggingface.co/CompVis/stable-diffusion-v1-4\n",
        "!git lfs clone https://$huggingface_username:$huggingface_token@huggingface.co/CompVis/stable-diffusion-v-1-4-original\n",
        "!git clone --recursive https://github.com/crowsonkb/k-diffusion.git\n",
        "!mkdir diffusers_output\n",
        "\n",
        "##### You should install other things here\n",
        "!pip install -U git+https://github.com/huggingface/diffusers.git\n",
        "!git clone https://github.com/DamascusGit/stable-diffusion.git\n",
        "\n",
        "##### Configure the environments here\n",
        "!mamba env update -n base -f stable-diffusion/environment.yaml"
      ],
      "metadata": {
        "id": "Yej7lhMSD2Z8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3255055-bd55-454b-de94-8c24afcd55ac"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your HuggingFace Username: ··········\n",
            "Enter your HuggingFace Token: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NSFW Toggle Switch\n",
        "-----------\n",
        "I am obligated to prompt you to review the [Stable Diffusion Model Card](https://colab.research.google.com/drive/1jUwJ0owjigpG-9m6AI_wEStwimisUE17#scrollTo=PqBbJRDUpohR&line=3&uniqifier=1) on huggingface and consider the ethical / legal notices provided under the MIL and the Model Card with disabling the NSFW filters on Stable Diffusion, and advise you that:\n",
        "\n",
        "- You should never make a copy of this notebook that has had its safety checker stubbed or disabled available to the general public for safety and legal reasons, especially in a place where the source code of the version which has the NSFW filter disabled may fall into the hands of a minor.\n",
        "- Every country's laws differ in the matter of what isn't and isn't okay in regards to NSFW contexts, and what defines a minor.  These laws apply in your jurisdication.\n",
        "- It is legal view adult content in my country (US) where the ability to disable the safety checker was developed, with restrictions, but may not be in yours, or may hold additional restrictions.  Please compose your prompts with caution and respect to these laws.\n",
        "\n",
        "You acknowledge and understand that:\n",
        "\n",
        "- Disabling this filter will reveal everything.\n",
        "- You hold yourself accountable for what you prompt for.\n",
        "- You acknowledge that as an AI, it isn't perfect, and sometimes, it might return some f\\*\\*ked up shock content without its filters on."
      ],
      "metadata": {
        "id": "PqBbJRDUpohR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_imports\n",
        "\n",
        "DISABLE_NSFW = False #@param {type:\"boolean\"}\n",
        "\n",
        "# Disable NSFW check.\n",
        "if DISABLE_NSFW == True:\n",
        "  with open('/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py','w') as file:\n",
        "    file.write('''import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from transformers import CLIPConfig, CLIPVisionModel, PreTrainedModel\n",
        "\n",
        "class StableDiffusionSafetyChecker(PreTrainedModel):\n",
        "    config_class = CLIPConfig\n",
        "\n",
        "    def __init__(self, config: CLIPConfig):\n",
        "        super().__init__(config)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, clip_input, images):\n",
        "        return images, False''')"
      ],
      "metadata": {
        "id": "P8gV4-qRDn1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db8d7e6d-86df-48c6-a655-1ed01ac6a6dd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Failed to call git rev-parse --git-dir --show-toplevel: \"fatal: not a git repository (or any of the parent directories): .git\\n\"\n",
            "Git LFS initialized.\n",
            "WARNING: 'git lfs clone' is deprecated and will not be updated\n",
            "          with new flags from 'git clone'\n",
            "\n",
            "'git clone' has been updated in upstream Git to have comparable\n",
            "speeds to 'git lfs clone'.\n",
            "fatal: destination path 'stable-diffusion-v1-4' already exists and is not an empty directory.\n",
            "Error(s) during clone:\n",
            "git clone failed: exit status 128\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/huggingface/diffusers.git\n",
            "  Cloning https://github.com/huggingface/diffusers.git to /tmp/pip-req-build-_wc5vq_7\n",
            "  Running command git clone -q https://github.com/huggingface/diffusers.git /tmp/pip-req-build-_wc5vq_7\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from diffusers==0.2.4) (1.12.1+cu113)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from diffusers==0.2.4) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from diffusers==0.2.4) (3.8.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from diffusers==0.2.4) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from diffusers==0.2.4) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from diffusers==0.2.4) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from diffusers==0.2.4) (4.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from diffusers==0.2.4) (0.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.8.1->diffusers==0.2.4) (4.1.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.8.1->diffusers==0.2.4) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.8.1->diffusers==0.2.4) (6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.8.1->diffusers==0.2.4) (4.64.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub<1.0,>=0.8.1->diffusers==0.2.4) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->diffusers==0.2.4) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->diffusers==0.2.4) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->diffusers==0.2.4) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->diffusers==0.2.4) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->diffusers==0.2.4) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.21.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "mkdir: cannot create directory ‘diffusers_output’: File exists\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.64.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2022.6.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.24.57)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.1.1)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (1.0.1)\n",
            "Requirement already satisfied: botocore<1.28.0,>=1.27.57 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (1.27.57)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.6.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.57->boto3->pytorch-pretrained-bert) (1.25.11)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.57->boto3->pytorch-pretrained-bert) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.57->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.1)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (6.1.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.9.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 27.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.7)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.21.6)\n",
            "fatal: destination path 'k-diffusion' already exists and is not an empty directory.\n",
            "\n",
            "        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "        To login, `huggingface_hub` now requires a token generated from https://huggingface.co/settings/tokens .\n",
            "        \n",
            "/usr/lib/python3.7/getpass.py:91: GetPassWarning: Can not control echo on the terminal.\n",
            "  passwd = fallback_getpass(prompt, stream)\n",
            "Warning: Password input may be echoed.\n",
            "Token: \n",
            "Login successful\n",
            "Your token has been saved to /root/.huggingface/token\n",
            "\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n",
            "\n",
            "git config --global credential.helper store\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at /root/.cache/huggingface/diffusers/models--CompVis--stable-diffusion-v1-4/snapshots/2881c082ee0dc70d9eeb645f1b150040a4b62767/safety_checker were not used when initializing StableDiffusionSafetyChecker: ['vision_model.vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.vision_model.post_layernorm.weight', 'vision_model.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.3.mlp.fc2.bias', 'concept_embeds_weights', 'vision_model.vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.vision_model.post_layernorm.bias', 'vision_model.vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.vision_model.pre_layrnorm.weight', 'vision_model.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.vision_model.embeddings.patch_embedding.weight', 'vision_model.vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.23.layer_norm2.weight', 'special_care_embeds_weights', 'vision_model.vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.23.mlp.fc2.bias', 'special_care_embeds', 'vision_model.vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'concept_embeds', 'vision_model.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'visual_projection.weight', 'vision_model.vision_model.pre_layrnorm.bias', 'vision_model.vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.vision_model.embeddings.position_embedding.weight', 'vision_model.vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.vision_model.embeddings.position_ids', 'vision_model.vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.vision_model.embeddings.class_embedding', 'vision_model.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.3.layer_norm1.weight']\n",
            "- This IS expected if you are initializing StableDiffusionSafetyChecker from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing StableDiffusionSafetyChecker from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save GPU VRAM\n",
        "\n",
        "This setting can help reduce Out of Memory problems.\n",
        "\n",
        "> **NOTICE:** If you are limited by GPU memory and have less than 10GB of GPU RAM available, please make sure to load the StableDiffusionPipeline in float16 precision."
      ],
      "metadata": {
        "id": "VHaZZ0uKti1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_imports\n",
        "\n",
        "SAVE_GPU_VRAM = False #@param {type:\"boolean\"}\n",
        "\n",
        "# lms = LMSDiscreteScheduler(\n",
        "#     beta_start=0.00085, \n",
        "#     beta_end=0.012, \n",
        "#     beta_schedule=\"scaled_linear\"\n",
        "# )\n",
        "\n",
        "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "\n",
        "pipe = None\n",
        "if SAVE_GPU_VRAM == True:\n",
        "  pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\")\n",
        "else:\n",
        "  pipe = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token=True).to(\"cuda\")"
      ],
      "metadata": {
        "id": "n0IEImJ1KRWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Settings\n",
        "\n",
        "Here you can configure your prompt settings."
      ],
      "metadata": {
        "id": "cfm3a3NvSX-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_imports\n",
        "\n",
        "PROMPT = \"beautiful spanish castle at sunset on a hill with high spires at sunset, painting, traditional artwork\" #@param {type:'string'}\n",
        "STEPS = 50 #@param {type:\"slider\", min:30, max:200, step:5} \n",
        "CFG_SCALE = 19.1 #@param {type:\"slider\", min:0, max:25, step:0.1}\n",
        "WIDTH = 512 #@param {type:\"slider\", min:64, max:1280, step:64}\n",
        "HEIGHT = 512 #@param {type:\"slider\", min:64, max:1280, step:64}\n",
        "SEED = 0 #@param {type:'integer'}\n",
        "NUM_ITERS = 25 #@param {type:\"slider\", min:1, max:100, step:1} \n",
        "PRECISION = \"autocast\" #@param [\"full\",\"autocast\"]\n",
        "\n",
        "precision_scope = autocast if PRECISION==\"autocast\" else nullcontext\n",
        "ORIG_SEED = SEED"
      ],
      "metadata": {
        "id": "Ucr5_i21xSjv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395,
          "referenced_widgets": [
            "9aefb1718bcd47dc8065cdf14298ebda",
            "7481fcd5b0344d07aac01d97dec25022",
            "5e01953210814bdfa6d39060027210e4",
            "d25769c46b2d48b0803e4fc29787787c",
            "56e5ae7ca6f2411dbfd8bbb591e1612d",
            "6bf6a7039da642c59107a7614dde0a3c",
            "abf7dd85ce7c448abb81014d5928b33d",
            "ebb0ccdd3ff546318f242221d11c8a7e",
            "55fb96a9135b4475a4285dfadb726861",
            "103e359758204b93b01c8c068f701abd",
            "25ede9b1827a4c258ca6b85996ee35f6"
          ]
        },
        "outputId": "486f262a-2b43-4668-c08f-d2c5d7417583"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: 3349957564, Scale: 19.1, Steps: 150\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9aefb1718bcd47dc8065cdf14298ebda"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/d2e234f7cc04bf79/manager.min.js"
                }
              }
            }
          }
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-2b7d654cd0e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Seed: {SEED}, Scale: {SCALE}, Steps: {STEPS}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPROMPT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_inference_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWIDTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHEIGHT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguidance_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sample\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, prompt, height, width, num_inference_steps, guidance_scale, eta, generator, output_type, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;31m# run safety checker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0msafety_cheker_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy_to_pil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_nsfw_concept\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafety_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafety_cheker_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pil\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, clip_input, images)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'false' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Content Path Settings\n",
        "\n",
        "Here you can configure saving / loading from Content Folder & Google Drive."
      ],
      "metadata": {
        "id": "c8wtXDRcO3Hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_imports\n",
        "\n",
        "CONTENT_ENABLE = False #@param {type:\"boolean\"}\n",
        "GOOGLE_DRIVE_ENABLE = False #@param {type:\"boolean\"}\n",
        "PROMPT_LOG_ENABLE = False #@param {type:\"boolean\"}\n",
        "\n",
        "CONTENT_MOUNT = \"/content\" #@param {type:\"string\"}\n",
        "CONTENT_SAVE_PATH  = \"/images\" #@param {type:\"string\"}\n",
        "CONTENT_PROMPT_PATH  = \"/prompts\" #@param {type:\"string\"}\n",
        "\n",
        "GOOGLE_DRIVE_MOUNT = \"/mnt/gdrive/MyDrive\" #@param {type:\"string\"}\n",
        "GOOGLE_DRIVE_SAVE_PATH = \"/images\" #@param {type:\"string\"}\n",
        "GOOGLE_DRIVE_PROMPT_PATH  = \"/prompts\" #@param {type:\"string\"}\n",
        "\n",
        "# func_mounthandler - a function that should return a boolean True when mounted success.\n",
        "# func_ismountedhandler - a function that should return a boolean if mountpoint is mounted.\n",
        "def mount_pathConstruct(enabled: bool, mountpoint: str, subpath: str, func_mounthandler: function = None, func_ismountedhandler: function = None):\n",
        "  if enabled == True:\n",
        "    try:\n",
        "      if func_mounthandler != None:\n",
        "        if func_ismountedhandler == None:\n",
        "          raise Exception(f\"Cannot mount: {mountpoint}, no func_ismountedhandler specified.\")\n",
        "        if not func_ismountedhandler(mountpoint):\n",
        "          result = func_mounthandler(mountpoint, subpath)\n",
        "          if not result:\n",
        "            raise Exception(f\"Failed to mount: {mountpoint}\")\n",
        "      if not os.path.exists(f\"{mountpoint}{subpath}\"):\n",
        "        !mkdir -p ${mountpoint}${subpath}\n",
        "    except Exception as err:\n",
        "      enable = False\n",
        "      print(f\"There was an error mounting {mountpoint}{subpath}: {err}\")\n",
        "\n",
        "if CONTENT_ENABLE:\n",
        "  mount_pathConstruct(CONTENT_ENABLE, CONTENT_MOUNT, CONTENT_SAVE_PATH, lambda x, lambda x: os.path.exists(CONTENT_MOUNT))\n",
        "  if PROMPT_LOG_ENABLE:\n",
        "    mount_pathConstruct(CONTENT_ENABLE, CONTENT_MOUNT, CONTENT_PROMPT_PATH)\n",
        "\n",
        "if GOOGLE_DRIVE_ENABLE:\n",
        "  drive.mount(GOOGLE_DRIVE_MOUNT)\n",
        "  mount_pathConstruct(GOOGLE_DRIVE_ENABLE, GOOGLE_DRIVE_MOUNT, GOOGLE_DRIVE_SAVE_PATH)\n",
        "  if PROMPT_LOG_ENABLE:\n",
        "    mount_pathConstruct(GOOGLE_DRIVE_ENABLE, GOOGLE_DRIVE_MOUNT, GOOGLE_DRIVE_PROMPT_PATH)\n",
        "\n",
        "else:\n",
        "  OUT_PATH_SAVE = '/content/unet_output'"
      ],
      "metadata": {
        "id": "pKUX-ZXdfqE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diffusion Method\n",
        "\n",
        "This method runs via basic stable diffusion using the huggingface diffuser library.  Nothing special, good vanilla baseline."
      ],
      "metadata": {
        "id": "mQTuNJukMuwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_imports\n",
        "\n",
        "def stable_diffusion(prompt: str, scale: float, steps: float, seed: Long, width, height, generator):\n",
        "  epoch_time = int(time.time())\n",
        "  if CONTENT_\n",
        "  with open(f'{OUTDIR}/{epoch_time}_prompt.txt', 'w') as file:\n",
        "    file.write(prompt)\n",
        "\n",
        "  with precision_scope(\"cuda\"):\n",
        "    for iteration in range(NUM_ITERS):\n",
        "      \n",
        "      if ORIG_SEED == 0:\n",
        "        rand_num = random.randint(0,4294967295)\n",
        "        gen_seed = torch.Generator(\"cuda\").manual_seed(rand_num)\n",
        "      else:\n",
        "        gen_seed = torch.Generator(\"cuda\").manual_seed(SEED)\n",
        "      epoch_time = int(time.time())\n",
        "      try:\n",
        "        print(f'Seed: {rand_num}, Scale: {SCALE}, Steps: {STEPS}')\n",
        "      except NameError:\n",
        "        print(f'Seed: {SEED}, Scale: {SCALE}, Steps: {STEPS}')\n",
        "      \n",
        "      image = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=gen_seed)[\"sample\"][0]  \n",
        "      display(image)\n",
        "      try:\n",
        "        image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{rand_num}.png')\n",
        "      except NameError:\n",
        "        image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}.png')\n",
        "  \n",
        "  stable_diffusion()"
      ],
      "metadata": {
        "id": "r4SUOj1jM6aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## How to run Offline\n",
        "\n",
        "Download and install:\n",
        "https://developer.nvidia.com/cuda-11-6-2-download-archive and\n",
        "https://pytorch.org/get-started/locally/\n",
        "\n",
        "in cmd or terminal:\n",
        "\n",
        "```\n",
        "git lfs install\n",
        "GIT_LFS_SKIP_SMUDGE=0\n",
        "git lfs clone https://<huggingface_username>:<huggingface_token>@huggingface.co/CompVis/stable-diffusion-v1-4\n",
        "echo <huggingface_token> | huggingface-cli login\n",
        "pip install -U git+https://github.com/huggingface/diffusers.git\n",
        "pip install transformers\n",
        "mkdir diffusers_output\n",
        "pip install pytorch-pretrained-bert\n",
        "pip install spacy ftfy\n",
        "python -m spacy download en\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "with python 3.7.2+:\n",
        "from torch import autocast\n",
        "import random\n",
        "import torch\n",
        "from contextlib import contextmanager, nullcontext\n",
        "import time\n",
        "from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
        "from PIL import Image\n",
        "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token=True).to(\"cuda\")\n",
        "OUTDIR = 'diffusers_output'\n",
        "\n",
        "PROMPT = \"stylized digital illustration of a subterranean magic castle surrounded by a moat, by john berkey, magic the gathering art \"\n",
        "NUM_ITERS = 5\n",
        "SEED = 0\n",
        "SCALE = 14\n",
        "WIDTH = 704\n",
        "HEIGHT = 512\n",
        "STEPS = 150\n",
        "\n",
        "\n",
        "ORIG_SEED = SEED\n",
        "\n",
        "epoch_time = int(time.time())\n",
        "with open(f'{OUTDIR}/{epoch_time}_prompt.txt', 'w') as file:\n",
        "    file.write(PROMPT)\n",
        "with autocast(\"cuda\"):\n",
        "    for iteration in range(NUM_ITERS):\n",
        "        if ORIG_SEED == 0:\n",
        "            rand_num = random.randint(0, 4294967295)\n",
        "            gen_seed = torch.Generator(\"cuda\").manual_seed(rand_num)\n",
        "        else:\n",
        "            gen_seed = torch.Generator(\"cuda\").manual_seed(SEED)\n",
        "        epoch_time = int(time.time())\n",
        "        print(f'Filename: {str(epoch_time)}_scale_{-+SCALE}_steps_{STEPS}_seed_{rand_num}.png')\n",
        "        print(f'Seed: {rand_num}, Scale: {SCALE}, Steps: {STEPS}')\n",
        "\n",
        "        image = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE,\n",
        "                     generator=gen_seed)[\"sample\"][0]\n",
        "\n",
        "        image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{rand_num}.png')\n",
        "        img = Image.open(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{rand_num}.png')\n",
        "        img.show()\n",
        "```"
      ],
      "metadata": {
        "id": "Gr_GaJY7Ldq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uHhWRt2KOzt5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TXT2IMG Method"
      ],
      "metadata": {
        "id": "WcnY9hUsy76f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_imports\n",
        "\n",
        "\n",
        "if SEED == 0:\n",
        "  SEED = random.randint(0, 4294967295)\n",
        "condacolab.install()\n",
        "root_code = root_model = \"/content/stableai\"\n",
        "code_dir = root_code + \"/stable-diffusion\"\n",
        "if not os.path.isdir(root_code):\n",
        "  !mkdir $root_code\n",
        "%cd $root_code\n",
        "if not os.path.isdir(root_model):\n",
        "  !mkdir $root_model\n",
        "%cd $root_model\n",
        "!git lfs install\n",
        "!GIT_LFS_SKIP_SMUDGE=0\n",
        "# Will take a long time\n",
        "def display_last_grid(grid_dir):\n",
        "  dir_list = os.listdir(grid_dir)\n",
        "  dir_list.sort()\n",
        "  #print (dir_list)\n",
        "  last_image = dir_list[-2]\n",
        "  img = Image.open(grid_dir + \"/\" + last_image).convert('RGB')\n",
        "  target_size = 600\n",
        "  img.thumbnail((target_size,target_size))\n",
        "  display (img)\n",
        "!mkdir /content/txt2img_output\n",
        "%cd $code_dir"
      ],
      "metadata": {
        "id": "-HMZ9IiRDs8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set Image Generation Parameters\n",
        "run_imports\n",
        "root_code = root_model = \"/content/stableai\"\n",
        "code_dir = root_code + \"/stable-diffusion\"\n",
        "\n",
        "if USE_DDIM:\n",
        "  PLMS = f\"--ddim_eta {DDIM_ETA}\"\n",
        "else:\n",
        "  PLMS = \"--plms\"\n",
        "if SKIP_SAVE:\n",
        "  SKIP_CONF = \"--skip_save\"\n",
        "else:\n",
        "  SKIP_CONF = \"\"\n",
        "if USE_LAION400M:\n",
        "  LAION_CONF = \"--laion400m\"\n",
        "else:\n",
        "  LAION_CONF = \"\"\n",
        "%cd $code_dir\n",
        "def display_last_grid(grid_dir):\n",
        "  dir_list = os.listdir(grid_dir)\n",
        "  dir_list.sort()\n",
        "  #print (dir_list)\n",
        "  last_image = dir_list[-2]\n",
        "  img = Image.open(grid_dir + \"/\" + last_image).convert('RGB')\n",
        "  target_size = 600\n",
        "  img.thumbnail((target_size,target_size))\n",
        "  display (img)\n",
        "print (\"Starting to generate \" + str(NUM_ITERS * NUM_SAMPLES) + \" images\")\n",
        "model_ckpt = f'{root_model}/stable-diffusion-v-1-4-original/{CHECKPOINT}'\n",
        "import time\n",
        "epoch_time = int(time.time())\n",
        "try:\n",
        "  with open(f'{OUTDIR}/samples/{epoch_time}_promptinput_seed_{SEED}.txt', 'w') as file:\n",
        "    file.write(PROMPT)\n",
        "except FileNotFoundError:\n",
        "  !mkdir $OUTDIR/samples/\n",
        "PROMPT = PROMPT.strip('\"')\n",
        "!python scripts/txt2img.py --C $LATENT_SAMPLES --f $DOWNSAMPLING_FACTOR --seed $SEED $PLMS $SKIP_CONF $LAION_CONF --prompt \"$PROMPT\" --ddim_step $STEPS --W $WIDTH --H $HEIGHT --n_samples $NUM_SAMPLES --n_iter $NUM_ITERS --ddim_step $STEPS --outdir $OUTDIR --ckpt $model_ckpt --precision $PRECISION --scale $SCALE\n",
        "try:\n",
        "  display_last_grid (OUTDIR)\n",
        "except Exception:\n",
        "  pass"
      ],
      "metadata": {
        "id": "9QnhfmAM0t-X"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Stable Diffusion Colab v0.17 (with new 1.4 weights + diffusers & txt2img are now working without NSFW)",
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9aefb1718bcd47dc8065cdf14298ebda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7481fcd5b0344d07aac01d97dec25022",
              "IPY_MODEL_5e01953210814bdfa6d39060027210e4",
              "IPY_MODEL_d25769c46b2d48b0803e4fc29787787c"
            ],
            "layout": "IPY_MODEL_56e5ae7ca6f2411dbfd8bbb591e1612d"
          }
        },
        "7481fcd5b0344d07aac01d97dec25022": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bf6a7039da642c59107a7614dde0a3c",
            "placeholder": "​",
            "style": "IPY_MODEL_abf7dd85ce7c448abb81014d5928b33d",
            "value": ""
          }
        },
        "5e01953210814bdfa6d39060027210e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebb0ccdd3ff546318f242221d11c8a7e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_55fb96a9135b4475a4285dfadb726861",
            "value": 1
          }
        },
        "d25769c46b2d48b0803e4fc29787787c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_103e359758204b93b01c8c068f701abd",
            "placeholder": "​",
            "style": "IPY_MODEL_25ede9b1827a4c258ca6b85996ee35f6",
            "value": " 168/? [01:46&lt;00:00,  1.62it/s]"
          }
        },
        "56e5ae7ca6f2411dbfd8bbb591e1612d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bf6a7039da642c59107a7614dde0a3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abf7dd85ce7c448abb81014d5928b33d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ebb0ccdd3ff546318f242221d11c8a7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "55fb96a9135b4475a4285dfadb726861": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "103e359758204b93b01c8c068f701abd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25ede9b1827a4c258ca6b85996ee35f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
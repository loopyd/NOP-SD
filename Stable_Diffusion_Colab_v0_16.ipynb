{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/loopyd/NOP-SD/blob/main/Stable_Diffusion_Colab_v0_16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Stable Diffusion Colab v0.16\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "J01cBPX_O0wF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tring to make this a one-stop shop for various programs + a GOTO guide on how to install everything locally. If you have suggestions, bug reports, or implementations, feel free to contact me on Discord. NOP#1337\n",
        "\n",
        "Changelog:\n",
        "- v0.1: Colab creation\n",
        "- v0.11: Google Drive option for TXT2IMG and some error corrections\n",
        "- v0.12: Added more options to TXT2IMG\n",
        "- v0.13: Diffusers added a feature which broke the pipeline with the current implementation, reverted back to an older version\n",
        "- v0.14: Added in full precision in the diffuser method\n",
        "- v0.15: Added in file saving in drive for diffusers\n",
        "- v0.16: Added in prompt saving\n",
        "- v0.17: Added in the new weights into diffusers and disabled the NSFW check (optional)\n",
        "\n",
        "TODO:\n",
        "- Add offline install instructions\n",
        "- Add in k-diffusion\n",
        "- Add in inpainting\n",
        "- Add init functionality\n",
        "- Several prompts in one run\n",
        "- Add in an upscaler\n",
        "- Add in a plms, ddim, etc choice (50% done)\n",
        "- Option to load a config file to load in preset settings\n",
        "- More functionality\n",
        "\n",
        "By NOP#1337\n",
        "\n",
        "Credits:\n",
        "\n",
        "- Pierre#2575 and Ragnall#1177 for the default prompt\n",
        "- Original TXT2IMG Notebook: Lucas Ferreira da Silva, Madams, Greg Turk\n",
        "\n",
        "\n",
        "\n",
        "#**IMPORTANT**\n",
        "\n",
        "You will need a huggingface account, username, and token.\n",
        "Grab your token from: https://huggingface.co/settings/tokens\n",
        "It should be the \"User Access Token\"\n"
      ],
      "metadata": {
        "id": "_Y6RXjS1tTji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPU Info"
      ],
      "metadata": {
        "id": "o5V9MWbFtpNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "_ekR-LW6trWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diffusers Method"
      ],
      "metadata": {
        "id": "VHaZZ0uKti1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "huggingface_username = getpass('Enter your HuggingFace Username: ')\n",
        "huggingface_token = getpass('Enter your HuggingFace Token: ')\n",
        "\n"
      ],
      "metadata": {
        "id": "Yej7lhMSD2Z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "!git lfs install\n",
        "!GIT_LFS_SKIP_SMUDGE=0\n",
        "# This will take a while\n",
        "!git lfs clone https://$huggingface_username:$huggingface_token@huggingface.co/CompVis/stable-diffusion-v1-4\n",
        "!echo $huggingface_token | huggingface-cli login\n",
        "!pip install -U git+https://github.com/huggingface/diffusers.git\n",
        "!pip install transformers\n",
        "# make sure you're logged in with `huggingface-cli login`\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
        "\n",
        "# lms = LMSDiscreteScheduler(\n",
        "#     beta_start=0.00085, \n",
        "#     beta_end=0.012, \n",
        "#     beta_schedule=\"scaled_linear\"\n",
        "# )\n",
        "\n",
        "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "\n",
        "!mkdir diffusers_output\n",
        "!pip install pytorch-pretrained-bert\n",
        "!pip install spacy ftfy\n",
        "!python -m spacy download en\n",
        "!pip install scipy\n",
        "!git clone --recursive https://github.com/crowsonkb/k-diffusion.git\n",
        "try:\n",
        "  pipe = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token=True).to(\"cuda\")\n",
        "except OSError:\n",
        "  !echo $huggingface_token | huggingface-cli login\n",
        "  pipe = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token=True).to(\"cuda\")"
      ],
      "metadata": {
        "id": "P8gV4-qRDn1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "from contextlib import contextmanager, nullcontext\n",
        "import time\n",
        "\n",
        "PROMPT = \"stylized digital illustration of a subterranean magic castle surrounded by a moat, by john berkey, magic the gathering art \" #@param {type:'string'}\n",
        "STEPS = 145 #@param {type:\"slider\", min:5, max:500, step:5} \n",
        "SEED = 0 #@param {type:'integer'}\n",
        "NUM_ITERS = 7 #@param {type:\"slider\", min:1, max:100, step:1} \n",
        "WIDTH = 512 #@param {type:\"slider\", min:512, max:1920, step:64}\n",
        "HEIGHT = 512 #@param {type:\"slider\", min:512, max:1920, step:64}\n",
        "SCALE = 13.8 #@param {type:\"slider\", min:0, max:25, step:0.1}\n",
        "PRECISION = \"autocast\" #@param [\"full\",\"autocast\"]\n",
        "USE_DRIVE_FOR_PICS = True #@param {type:\"boolean\"}\n",
        "DRIVE_PIC_DIR = \"AI_PICS\" #@param {type:\"string\"}\n",
        "precision_scope = autocast if PRECISION==\"autocast\" else nullcontext\n",
        "ORIG_SEED = SEED\n",
        "\n",
        "if USE_DRIVE_FOR_PICS and not os.path.exists('/content/drive'):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "if USE_DRIVE_FOR_PICS and not os.path.exists(f'/content/drive/MyDrive/{DRIVE_PIC_DIR}'):\n",
        "  !mkdir /content/drive/MyDrive/$DRIVE_PIC_DIR\n",
        "if USE_DRIVE_FOR_PICS:\n",
        "  OUTDIR = f'/content/drive/MyDrive/{DRIVE_PIC_DIR}'\n",
        "else:\n",
        "  OUTDIR = '/content/diffusers_output'\n",
        "epoch_time = int(time.time())\n",
        "with open(f'{OUTDIR}/{epoch_time}_prompt.txt', 'w') as file:\n",
        "      file.write(PROMPT)\n",
        "with precision_scope(\"cuda\"):\n",
        "  for iteration in range(NUM_ITERS):\n",
        "    \n",
        "    if ORIG_SEED == 0:\n",
        "      rand_num = random.randint(0,4294967295)\n",
        "      gen_seed = torch.Generator(\"cuda\").manual_seed(rand_num)\n",
        "    else:\n",
        "      gen_seed = torch.Generator(\"cuda\").manual_seed(SEED)\n",
        "    epoch_time = int(time.time())\n",
        "    print(f'Filename: {str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{rand_num}.png')\n",
        "    try:\n",
        "      print(f'Seed: {rand_num}, Scale: {SCALE}, Steps: {STEPS}')\n",
        "    except NameError:\n",
        "      print(f'Seed: {SEED}, Scale: {SCALE}, Steps: {STEPS}')\n",
        "    \n",
        "    image = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=gen_seed)[\"sample\"][0]  \n",
        "    display(image)\n",
        "    try:\n",
        "      image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{rand_num}.png')\n",
        "    except NameError:\n",
        "      image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}.png')"
      ],
      "metadata": {
        "id": "Ucr5_i21xSjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##If you want to disable NSFW filter, run this"
      ],
      "metadata": {
        "id": "UP2BFExwHSwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py','w') as file:\n",
        "  file.write('''import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from transformers import CLIPConfig, CLIPVisionModel, PreTrainedModel\n",
        "\n",
        "from ...utils import logging\n",
        "\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "\n",
        "def cosine_distance(image_embeds, text_embeds):\n",
        "    normalized_image_embeds = nn.functional.normalize(image_embeds)\n",
        "    normalized_text_embeds = nn.functional.normalize(text_embeds)\n",
        "    return torch.mm(normalized_image_embeds, normalized_text_embeds.T)\n",
        "\n",
        "\n",
        "class StableDiffusionSafetyChecker(PreTrainedModel):\n",
        "    config_class = CLIPConfig\n",
        "\n",
        "    def __init__(self, config: CLIPConfig):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.vision_model = CLIPVisionModel(config.vision_config)\n",
        "        self.visual_projection = nn.Linear(config.vision_config.hidden_size, config.projection_dim, bias=False)\n",
        "\n",
        "        self.concept_embeds = nn.Parameter(torch.ones(17, config.projection_dim), requires_grad=False)\n",
        "        self.special_care_embeds = nn.Parameter(torch.ones(3, config.projection_dim), requires_grad=False)\n",
        "\n",
        "        self.register_buffer(\"concept_embeds_weights\", torch.ones(17))\n",
        "        self.register_buffer(\"special_care_embeds_weights\", torch.ones(3))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, clip_input, images):\n",
        "        pooled_output = self.vision_model(clip_input)[1]  # pooled_output\n",
        "        image_embeds = self.visual_projection(pooled_output)\n",
        "\n",
        "        special_cos_dist = cosine_distance(image_embeds, self.special_care_embeds).cpu().numpy()\n",
        "        cos_dist = cosine_distance(image_embeds, self.concept_embeds).cpu().numpy()\n",
        "\n",
        "        result = []\n",
        "        batch_size = image_embeds.shape[0]\n",
        "        for i in range(batch_size):\n",
        "            result_img = {\"special_scores\": {}, \"special_care\": [], \"concept_scores\": {}, \"bad_concepts\": []}\n",
        "\n",
        "            # increase this value to create a stronger `nfsw` filter\n",
        "            # at the cost of increasing the possibility of filtering benign images\n",
        "            adjustment = 0.0\n",
        "\n",
        "            for concet_idx in range(len(special_cos_dist[0])):\n",
        "                concept_cos = special_cos_dist[i][concet_idx]\n",
        "                concept_threshold = self.special_care_embeds_weights[concet_idx].item()\n",
        "                result_img[\"special_scores\"][concet_idx] = round(concept_cos - concept_threshold + adjustment, 3)\n",
        "                if result_img[\"special_scores\"][concet_idx] > 0:\n",
        "                    result_img[\"special_care\"].append({concet_idx, result_img[\"special_scores\"][concet_idx]})\n",
        "                    adjustment = 0.01\n",
        "\n",
        "            for concet_idx in range(len(cos_dist[0])):\n",
        "                concept_cos = cos_dist[i][concet_idx]\n",
        "                concept_threshold = self.concept_embeds_weights[concet_idx].item()\n",
        "                result_img[\"concept_scores\"][concet_idx] = round(concept_cos - concept_threshold + adjustment, 3)\n",
        "                if result_img[\"concept_scores\"][concet_idx] > 0:\n",
        "                    result_img[\"bad_concepts\"].append(concet_idx)\n",
        "\n",
        "            result.append(result_img)\n",
        "\n",
        "        has_nsfw_concepts = [len(res[\"bad_concepts\"]) > 0 for res in result]\n",
        "\n",
        "        #for idx, has_nsfw_concept in enumerate(has_nsfw_concepts):\n",
        "        #    if has_nsfw_concept:\n",
        "        #        images[idx] = np.zeros(images[idx].shape)  # black image\n",
        "\n",
        "        if any(has_nsfw_concepts):\n",
        "            logger.warning(\n",
        "                \"Potential NSFW content was detected in one or more images. A black image will be returned instead.\"\n",
        "                \" Try again with a different prompt and/or seed.\"\n",
        "            )\n",
        "\n",
        "        return images, has_nsfw_concepts''')"
      ],
      "metadata": {
        "id": "89mgcrsYH6CW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## How to run Offline\n",
        "\n",
        "Download and install:\n",
        "https://developer.nvidia.com/cuda-11-6-2-download-archive and\n",
        "https://pytorch.org/get-started/locally/\n",
        "\n",
        "in cmd or terminal:\n",
        "\n",
        "```\n",
        "git lfs install\n",
        "GIT_LFS_SKIP_SMUDGE=0\n",
        "git lfs clone https://<huggingface_username>:<huggingface_token>@huggingface.co/CompVis/stable-diffusion-v1-4\n",
        "echo <huggingface_token> | huggingface-cli login\n",
        "pip install -U git+https://github.com/huggingface/diffusers.git\n",
        "pip install transformers\n",
        "mkdir diffusers_output\n",
        "pip install pytorch-pretrained-bert\n",
        "pip install spacy ftfy\n",
        "python -m spacy download en\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "with python 3.7.2+:\n",
        "from torch import autocast\n",
        "import random\n",
        "import torch\n",
        "from contextlib import contextmanager, nullcontext\n",
        "import time\n",
        "from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
        "from PIL import Image\n",
        "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token=True)\n",
        "OUTDIR = 'diffusers_output'\n",
        "\n",
        "PROMPT = \"stylized digital illustration of a subterranean magic castle surrounded by a moat, by john berkey, magic the gathering art \"\n",
        "NUM_ITERS = 5\n",
        "SEED = 0\n",
        "SCALE = 14\n",
        "WIDTH = 704\n",
        "HEIGHT = 512\n",
        "STEPS = 150\n",
        "\n",
        "\n",
        "ORIG_SEED = SEED\n",
        "\n",
        "epoch_time = int(time.time())\n",
        "with open(f'{OUTDIR}/{epoch_time}_prompt.txt', 'w') as file:\n",
        "    file.write(PROMPT)\n",
        "with autocast(\"cuda\"):\n",
        "    for iteration in range(NUM_ITERS):\n",
        "        if ORIG_SEED == 0:\n",
        "            rand_num = random.randint(0, 4294967295)\n",
        "            gen_seed = torch.Generator(\"cuda\").manual_seed(rand_num)\n",
        "        else:\n",
        "            gen_seed = torch.Generator(\"cuda\").manual_seed(SEED)\n",
        "        epoch_time = int(time.time())\n",
        "        print(f'Filename: {str(epoch_time)}_scale_{-+SCALE}_steps_{STEPS}_seed_{rand_num}.png')\n",
        "        print(f'Seed: {rand_num}, Scale: {SCALE}, Steps: {STEPS}')\n",
        "\n",
        "        image = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE,\n",
        "                     generator=gen_seed)[\"sample\"][0]\n",
        "\n",
        "        image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{rand_num}.png')\n",
        "        img = Image.open(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{rand_num}.png')\n",
        "        img.show()\n",
        "```\n",
        "\n",
        "If you want to disable the NSFW check, see above\n"
      ],
      "metadata": {
        "id": "Gr_GaJY7Ldq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TXT2IMG Method"
      ],
      "metadata": {
        "id": "WcnY9hUsy76f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "huggingface_username = getpass('Enter your HuggingFace Username: ')\n",
        "huggingface_token = getpass('Enter your HuggingFace Token: ')\n",
        "\n"
      ],
      "metadata": {
        "id": "BKZThsccD4Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "root_code = root_model = \"/content/stableai\"\n",
        "code_dir = root_code + \"/stable-diffusion\"\n",
        "import os\n",
        "if not os.path.isdir(root_code):\n",
        "  !mkdir $root_code\n",
        "%cd $root_code\n",
        "!git clone https://github.com/CompVis/stable-diffusion.git\n",
        "!mamba env update -n base -f stable-diffusion/environment.yaml\n",
        "!pip install torchmetrics==0.6.0\n",
        "!pip install kornia==0.6)\n",
        "import os\n",
        "if not os.path.isdir(root_model):\n",
        "  !mkdir $root_model\n",
        "%cd $root_model\n",
        "!git lfs install\n",
        "!GIT_LFS_SKIP_SMUDGE=0\n",
        "# Will take a long time\n",
        "!git lfs clone https://$huggingface_username:$huggingface_token@huggingface.co/CompVis/stable-diffusion-v-1-4-original\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "def display_last_grid(grid_dir):\n",
        "  dir_list = os.listdir(grid_dir)\n",
        "  dir_list.sort()\n",
        "  #print (dir_list)\n",
        "  last_image = dir_list[-2]\n",
        "  img = Image.open(grid_dir + \"/\" + last_image).convert('RGB')\n",
        "  target_size = 600\n",
        "  img.thumbnail((target_size,target_size))\n",
        "  display (img)\n",
        "!mkdir /content/txt2img_output\n",
        "%cd $code_dir"
      ],
      "metadata": {
        "id": "-HMZ9IiRDs8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set Image Generation Parameters\n",
        "root_code = root_model = \"/content/stableai\"\n",
        "code_dir = root_code + \"/stable-diffusion\"\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "PROMPT = \"stylized digital illustration of a subterranean magic castle surrounded by a moat, by john berkey, magic the gathering art \"   #@param {type:\"string\"}\n",
        "STEPS = 200 #@param {type:\"slider\", min:5, max:500, step:5} \n",
        "SEED = 0 #@param {type:'integer'}\n",
        "WIDTH = 704 #@param {type:\"slider\", min:512, max:1920, step:64}\n",
        "HEIGHT = 512 #@param {type:\"slider\", min:512, max:1920, step:64}\n",
        "SCALE = 13 #@param {type:\"slider\", min:0, max:50, step:0.5}\n",
        "NUM_SAMPLES = 1 #@param {type:\"slider\", min:1, max:4, step:1} \n",
        "NUM_ITERS = 1 #@param {type:\"slider\", min:1, max:20, step:1} \n",
        "CHECKPOINT = 'sd-v1-4.ckpt' #@param [\"sd-v1-4.ckpt\"]\n",
        "PRECISION = \"autocast\" #@param [\"full\",\"autocast\"]\n",
        "LATENT_SAMPLES = 4 #@param {type:\"slider\", min:1, max:20, step:1} \n",
        "DOWNSAMPLING_FACTOR = 8 #@param {type:\"slider\", min:1, max:20, step:1} \n",
        "USE_DDIM = True #@param {type:\"boolean\"}\n",
        "DDIM_ETA = 0.15 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "SKIP_SAVE = False #@param {type:\"boolean\"}\n",
        "USE_LAION400M = False #@param {type:\"boolean\"}\n",
        "USE_DRIVE_FOR_PICS = False #@param {type:\"boolean\"}\n",
        "DRIVE_PIC_DIR = \"AI_PICS\" #@param {type:\"string\"}\n",
        "# more possible image sizes: \n",
        "# 64 128 192 256 320 384 448 512 576 640 704 768 832 896 960 1024 1088 1152 1216 1280\n",
        "import os\n",
        "import random\n",
        "if SEED == 0:\n",
        "  SEED = random.randint(0, 4294967295)\n",
        "if USE_DRIVE_FOR_PICS and not os.path.exists('/content/drive'):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "if USE_DRIVE_FOR_PICS and not os.path.exists(f'/content/drive/MyDrive/{DRIVE_PIC_DIR}'):\n",
        "  !mkdir /content/drive/MyDrive/$DRIVE_PIC_DIR\n",
        "if USE_DRIVE_FOR_PICS:\n",
        "  OUTDIR = f'/content/drive/MyDrive/{DRIVE_PIC_DIR}'\n",
        "else:\n",
        "  OUTDIR = '/content/txt2img_output'\n",
        "if USE_DDIM:\n",
        "  PLMS = f\"--ddim_eta {DDIM_ETA}\"\n",
        "else:\n",
        "  PLMS = \"--plms\"\n",
        "if SKIP_SAVE:\n",
        "  SKIP_CONF = \"--skip_save\"\n",
        "else:\n",
        "  SKIP_CONF = \"\"\n",
        "if USE_LAION400M:\n",
        "  LAION_CONF = \"--laion400m\"\n",
        "else:\n",
        "  LAION_CONF = \"\"\n",
        "%cd $code_dir\n",
        "def display_last_grid(grid_dir):\n",
        "  dir_list = os.listdir(grid_dir)\n",
        "  dir_list.sort()\n",
        "  #print (dir_list)\n",
        "  last_image = dir_list[-2]\n",
        "  img = Image.open(grid_dir + \"/\" + last_image).convert('RGB')\n",
        "  target_size = 600\n",
        "  img.thumbnail((target_size,target_size))\n",
        "  display (img)\n",
        "print (\"Starting to generate \" + str(NUM_ITERS * NUM_SAMPLES) + \" images\")\n",
        "model_ckpt = f'{root_model}/stable-diffusion-v-1-4-original/{CHECKPOINT}'\n",
        "import time\n",
        "epoch_time = int(time.time())\n",
        "try:\n",
        "  with open(f'{OUTDIR}/samples/{epoch_time}_promptinput_seed_{SEED}.txt', 'w') as file:\n",
        "    file.write(PROMPT)\n",
        "except FileNotFoundError:\n",
        "  !mkdir $OUTDIR/samples/\n",
        "PROMPT = PROMPT.strip('\"')\n",
        "!python scripts/txt2img.py --C $LATENT_SAMPLES --f $DOWNSAMPLING_FACTOR --seed $SEED $PLMS $SKIP_CONF $LAION_CONF --prompt \"$PROMPT\" --ddim_step $STEPS --W $WIDTH --H $HEIGHT --n_samples $NUM_SAMPLES --n_iter $NUM_ITERS --ddim_step $STEPS --outdir $OUTDIR --ckpt $model_ckpt --precision $PRECISION --scale $SCALE\n",
        "try:\n",
        "  display_last_grid (OUTDIR)\n",
        "except Exception:\n",
        "  pass"
      ],
      "metadata": {
        "id": "9QnhfmAM0t-X"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Stable Diffusion Colab v0.16",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}